DAY 0 --repo created and initial problems added.
Goal: 30 advanced problems in Python and SQL.

DAY 1 -- Python Logic + SQL Queries.
- Started with logic-focused Python practice: built a transaction summary function to separate income and expenses.
- Used dictionaries and learned to print formatted results using f-strings.
- Created SQL tables (`sales`, `custmer`) and practiced `JOIN`, `GROUP BY`, and aggregate functions.
- Faced issues with query structure — learned that aliases and aggregates must be used carefully.
- Goal for Day 2: Build a data analysis challenge combining both — clean data in Python, analyze results with SQL.

📅 Day 2 — Expense Tracker Insights

Focused on translating business logic into Python and SQL.

Built an Expense Tracker that separates income & expenses, calculates totals, and finds top 3 spending categories.

Strengthened understanding of loops, dictionaries, conditional logic, and formatted output with f-strings.

In SQL, practiced JOIN, GROUP BY, and HAVING clauses to mirror Python logic at the database level.

Faced challenges managing nested loops and aliasing in SQL; solved them by rewriting from the output backward.

Takeaway: Thinking in steps before coding gives clarity — syntax follows logic.

Next Goal: Integrate small visualizations with Matplotlib for every analytical script

📅 Day 3 — Sales Analysis

Shifted from basic logic to data-driven analysis using Pandas and SQL window functions.

Cleaned and validated data types, calculated regional & customer totals, and identified top performers.

Learned to use groupby, aggregation, ranking, and sorting for insight generation.

Practiced RANK(), DENSE_RANK(), and ROW_NUMBER() in SQL to replicate analytical behavior across tools.

Improved code readability through function-based design (clean_and_describe, totals_and_top_customers, etc.).

Takeaway: Real analysis comes from combining technical logic + business context.

Next Goal: Start automating summaries and exports (CSV / dashboard snippets).

### 📅 Day 4 — Mini Automation: Clean & Export Reports
- Built a small, reusable cleaning pipeline using pandas to standardize columns, handle missing values, and coerce types.
- Generated export-ready reports: `sales_by_region.csv`, `top_customers_by_sales.csv`, and `monthly_summary.csv`.
- Verified results with a simple bar chart to ensure visual sanity checks before sharing reports.
- Implemented the same logic in SQL queries to keep parity between data engineering and analytics layers.
- **Takeaway:** Automating cleaning + export reduces manual errors and makes routine reporting reproducible. Next: schedule the pipeline and add basic logging & tests.

## 📅 Day 5 — Data Transformation & Insight Extraction

**Python**
- Created a sales performance report with performance %, status flag, and regional pivot summaries.
- Strengthened understanding of pivot tables and conditional logic.
- Automated CSV export for reporting.

**SQL**
- Implemented performance logic using CASE and AVG functions.
- Practiced translating analytical logic from Python to SQL smoothly.

**Reflection**
- Transformation is where data becomes information.  
- The goal is to make numbers talk — not just exist in tables.  
- Today proved that logic + structure > syntax alone.

## 📅 Day 6 — From Data to Story: Visualizing Insights

**Python**
- Created a line plot comparing revenue, expenses, and profit.
- Identified the most profitable month using data-driven logic.
- Strengthened storytelling through clear visuals and annotations.

**SQL**
- Replicated logic for profit calculations and rankings.
- Practiced aggregation queries for summarizing business data.

**Reflection**
- Visualization bridges the gap between numbers and narratives.
- A graph can make insights intuitive — analysis is only complete when it communicates clearly.
- Today reminded me that data storytelling is the language of impact.

# Day 7 — End-to-End Case Study: Reflection

**What I built**
- Ingested a raw-ish orders table, cleaned inconsistent amounts and dates, and handled refunds.
- Computed business KPIs (total revenue, refunds, net), revenue by region, top customers, and monthly trends — both in Python and SQL.
- Exported cleaned data and visuals for reporting.

**What I learned**
- Real datasets are messy: strings like "NaN" and commas in numbers are common. Defensive parsing is key.
- Mirroring logic in SQL and Python increases confidence — if both match, the numbers are reliable.
- Small enrichment (customer → region mapping) can unlock better aggregations.
- Documentation & exported CSVs make work reproducible and shareable.

**Next steps**
- Scale the pipeline: create functions to process arbitrary CSVs.
- Add more robust error-handling for date parsing and currency formats.
- Convert insights into a short slide/report for non-technical stakeholders.

# Day 9 — Predictive Indicators: Forecasting the Future

**Focus:** bridging analysis and prediction through simple forecasting.

### Key Learnings
- Learned how to model linear relationships between month and sales using regression.
- SQL’s window functions helped derive growth trends and moving averages for comparison.
- Discovered that even simple linear models can add business foresight if used responsibly.
- Visualization clarified forecast trends better than raw numbers.

### Takeaway
Prediction isn’t about complexity — it’s about context. Even basic regression can inform strategy when combined with analytical judgment.

**Next Steps:**
- Add confidence intervals and seasonal adjustments.
- Automate forecast refresh using dynamic CSV imports.

# Day 8 — Predictive Indicators: Seeing What’s Next

*Focus:* bridging data analysis and forecasting through simple prediction models.

### 🧠 Key Learnings
- Introduced predictive thinking using regression.
- Derived growth and moving average insights through SQL.
- Understood model evaluation metrics (MAE, R²).
- Visualized future performance for quick stakeholder understanding.

### 💡 Takeaway
Forecasting is not guessing — it’s disciplined extrapolation of patterns.

*Next Steps*
- Add confidence intervals to forecast.
- Automate dynamic updates through scheduled scripts


# Day 10 — Anomaly Detection

**Focus:** Identifying and handling outliers that distort analysis.

### Key Takeaways
- Applied z-score and SQL statistical functions for anomaly detection.
- Built an anomaly flag pipeline for easy validation.
- Learned how one unusual data point can mislead KPIs and trend forecasts.

### Mindset Note
In analytics, truth often hides in anomalies. Great analysts question deviations, not just averages.

# Day 11 — Correlation & Root-Cause Analysis

**Focus:** Understanding relationships between business drivers.

### Key Learnings
- Computed correlation using both Python and SQL logic.
- Learned to identify which metric drives another.
- Shifted perspective from “What changed?” to “Why did it change?”

### Mindset Note
Data doesn’t whisper — it speaks clearly when you connect the right dots.


